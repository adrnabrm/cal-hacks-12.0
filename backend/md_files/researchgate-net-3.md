                   

                                    Attention Is All You Need | Request PDF    

Article

# Attention Is All You Need

*   June 2017

DOI:[10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)

Authors:

[![Ashish Vaswani](https://c5.rgstatic.net/m/448675030402/images/icons/icons/author-avatar.svg)](scientific-contributions/Ashish-Vaswani-2107454759)

[Ashish Vaswani](scientific-contributions/Ashish-Vaswani-2107454759)

[Ashish Vaswani](scientific-contributions/Ashish-Vaswani-2107454759)

*   This person is not on ResearchGate, or hasn't claimed this research yet.
    

[![Noam Shazeer](https://c5.rgstatic.net/m/448675030402/images/icons/icons/author-avatar.svg)](scientific-contributions/Noam-Shazeer-2059717778)

[Noam Shazeer](scientific-contributions/Noam-Shazeer-2059717778)

[Noam Shazeer](scientific-contributions/Noam-Shazeer-2059717778)

*   This person is not on ResearchGate, or hasn't claimed this research yet.
    

[![Niki Parmar](https://c5.rgstatic.net/m/448675030402/images/icons/icons/author-avatar.svg)](scientific-contributions/Niki-Parmar-2129043508)

[Niki Parmar](scientific-contributions/Niki-Parmar-2129043508)

[Niki Parmar](scientific-contributions/Niki-Parmar-2129043508)

*   This person is not on ResearchGate, or hasn't claimed this research yet.
    

[![Jakob Uszkoreit at Google Inc.](https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg)](profile/Jakob-Uszkoreit)

[Jakob Uszkoreit](profile/Jakob-Uszkoreit)

*   [Google Inc.](https://www.researchgate.net/institution/Google_Inc)

Show all 8 authorsHide

![Request Full-text Paper PDF](images/symbols/publication/publication_pdf_icon.png)

Request full-text PDF

To read the full-text of this research, you can request a copy directly from the authors.

Request full-text

[Download citation](https://www.researchgate.net/publication/317558625_Attention_Is_All_You_Need/citation/download)

Copy link Link copied

* * *

[

Request full-text

](https://www.researchgate.net/lite.research.ResearchResourcesSummary.requestFulltext.html?publicationUid=317558625&ev=su_requestFulltext)[

Download citation

](https://www.researchgate.net/publication/317558625_Attention_Is_All_You_Need/citation/download)

Copy link Link copied

To read the full-text of this research, you can request a copy directly from the authors.

[

Citations (145,566)



](publication/317558625_Attention_Is_All_You_Need#citations)[

References (36)



](publication/317558625_Attention_Is_All_You_Need#references)

## Abstract

The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

 ![ResearchGate Logo](images/icons/svgicons/researchgate-logo-white.svg)

**Discover the world's research**

*   25+ million members
*   160+ million publication pages
*   2.3+ billion citations

[Join for free](signup.SignUp.html)

## No full-text available

![Request Full-text Paper PDF](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)

To read the full-text of this research,  
you can request a copy directly from the authors.

Request full-text PDF

## Citations (145,566)

## References (36)

... Large Language Models (LLMs) are advanced AI systems primarily designed to process and generate human-like text, but their capabilities extend far beyond natural language tasks, enabling transformative applications across diverse domains, including education, research, software development, and more \[1,2\]. Taking advantage of massive datasets and neural network architectures-such as the transformer mechanism \[2\]-LLMs can analyse context, predict sequential elements (e.g., the next word or token), and produce coherent, contextually appropriate outputs. ...

... Large Language Models (LLMs) are advanced AI systems primarily designed to process and generate human-like text, but their capabilities extend far beyond natural language tasks, enabling transformative applications across diverse domains, including education, research, software development, and more \[1,2\]. Taking advantage of massive datasets and neural network architectures-such as the transformer mechanism \[2\]\-LLMs can analyse context, predict sequential elements (e.g., the next word or token), and produce coherent, contextually appropriate outputs. ...

... The "Transformer" in GPT refers to the architectural framework that powers modern LLMs, enabling their remarkable fluency and adaptability. This architecture revolutionized natural language processing by introducing the attention mechanism \[2\], which allows the model to understand relationships within a sequence and process context effectively. Without the "T," LLMs could not achieve the coherence and precision that define their generative abilities. ...

[An Introduction to Large Language Models in Education](publication/396899925_An_Introduction_to_Large_Language_Models_in_Education)

Chapter

Full-text available

*   Jun 2025

*   [![Eduardo Araujo Oliveira](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Eduardo Araujo Oliveira](https://www.researchgate.net/profile/Eduardo-Oliveira-9)

Large language models (LLMs) have become central to contemporary advancements in education. LLMs facilitate applications such as automated feedback, question generation, sentiment analysis, and multilingual accessibility. This chapter examines the mechanisms that underpin LLMs—namely, transformer architecture, pre-training, and generative abilities. Moreover, we present the diverse applications of LLMs in education, some of which will be covered in subsequent tutorial chapters in the present book. Lastly, we explore tools for interacting with LLMs, from beginner-friendly web interfaces to more advanced tools like APIs and frameworks such as the OpenAI API and Hugging Face’s Transformers.

[View](publication/396899925_An_Introduction_to_Large_Language_Models_in_Education)

Show abstract

... The unified approach reveals that different explanation methods make implicit assumptions about feature independence and baseline distributions, with SHAP's game-theoretic foundation providing principled handling of feature correlations prevalent in financial transaction data, where amounts, frequencies, and timing patterns exhibit substantial interdependencies \[7\].Attentionbased mechanisms in neural network architectures provide model-intrinsic explainability through learned relevance weights that explicitly model which input elements contribute most strongly to predictions. The Transformer architecture employs multi-head attention mechanisms computing attention scores through scaled dot-product attention formulation(Q, K, V) = softmax(QK^T/√d\_k)V, where queries Q, keys K, and values V represent learned linear projections of input embeddings and d\_k denotes key dimensionality, with scaling factor √d\_k preventing softmax saturation for large dimensionalities \[8\]. Empirical results on machine translation benchmarks demonstrate that Transformer models with 6-layer encoder-decoder architectures utilizing 8 parallel attention heads of dimension 64 achieve state-of-the-art BLEU scores of 28.4 on English-to-German translation and 41.8 on English-to-French translation, substantially outperforming recurrent and convolutional baseline architectures while requiring significantly reduced training time through enhanced parallelization \[8\]. ...

... The Transformer architecture employs multi-head attention mechanisms computing attention scores through scaled dot-product attention formulation(Q, K, V) = softmax(QK^T/√d\_k)V, where queries Q, keys K, and values V represent learned linear projections of input embeddings and d\_k denotes key dimensionality, with scaling factor √d\_k preventing softmax saturation for large dimensionalities \[8\]. Empirical results on machine translation benchmarks demonstrate that Transformer models with 6-layer encoder-decoder architectures utilizing 8 parallel attention heads of dimension 64 achieve state-of-the-art BLEU scores of 28.4 on English-to-German translation and 41.8 on English-to-French translation, substantially outperforming recurrent and convolutional baseline architectures while requiring significantly reduced training time through enhanced parallelization \[8\]. For fraud detection applications processing transaction sequences, multi-head attention enables simultaneous modeling of diverse temporal patterns, with different attention heads learning to focus on distinct transaction characteristics, including monetary amounts, merchant categories, geographic locations, and temporal intervals, providing an interpretable decomposition of model decision-making that compliance officers can validate against established fraud indicators \[8\]. ...

... Empirical results on machine translation benchmarks demonstrate that Transformer models with 6-layer encoder-decoder architectures utilizing 8 parallel attention heads of dimension 64 achieve state-of-the-art BLEU scores of 28.4 on English-to-German translation and 41.8 on English-to-French translation, substantially outperforming recurrent and convolutional baseline architectures while requiring significantly reduced training time through enhanced parallelization \[8\]. For fraud detection applications processing transaction sequences, multi-head attention enables simultaneous modeling of diverse temporal patterns, with different attention heads learning to focus on distinct transaction characteristics, including monetary amounts, merchant categories, geographic locations, and temporal intervals, providing an interpretable decomposition of model decision-making that compliance officers can validate against established fraud indicators \[8\]. The attention weight distributions offer quantitative measures of feature relevance, with visualization techniques revealing that fraud classification models typically concentrate 60% to 80% of attention mass on 3 to 5 critical transactions within historical sequences, enabling investigators to prioritize examination of specific events contributing most strongly to suspicious classifications \[8\]. ...

[AI-Driven Compliance Automation in Banking: A Hybrid Model Integrating Natural Language Processing and Knowledge Graphs](publication/396897238_AI-Driven_Compliance_Automation_in_Banking_A_Hybrid_Model_Integrating_Natural_Language_Processing_and_Knowledge_Graphs)

Article

Full-text available

*   Oct 2025

Maintaining regulatory compliance while detecting ever more complex fraud patterns via conventional rules-based systems presents unmatched difficulties for the financial services sector. The incorporation of understandable artificial intelligence approaches with hybrid architectures integrating knowledge graphs and natural language processing to automate compliance and fraud detection in banking is discussed in this article. Machine learning models show superior performance to conventional detection methods, but their black-box character goes against transparency and explainability regulations. Using transformer-based language models and heterogeneous graph neural networks, the hybrid design extracts semantic patterns from textual transaction data while encoding domain knowledge via structured knowledge representations. Using SHAP and attentional mechanisms, human-interpretable explanations that satisfy legislative obligations can be created while keeping identification accuracy. Regulatory compliance frameworks, including the GDPR and Basel Committee guidelines, provide openness requirements, yet execution issues with regard to clarity, specificity, adversarial robustness, and computational overhead persist. Deploying reliable artificial intelligence systems for financial compliance calls for balancing the conflicting needs of stakeholder trust, traceability performance, and operational efficiency by means of well-thought-out governance systems and multi-modal explainability strategies.

[View](publication/396897238_AI-Driven_Compliance_Automation_in_Banking_A_Hybrid_Model_Integrating_Natural_Language_Processing_and_Knowledge_Graphs)

Show abstract

... Nevertheless, persistent challenges remain, including the precise delineation of object boundaries, class imbalance issues, and the substantial demands on computational resources. Due to their global self-attention mechanism, transformers \[31,32\] are highly effective at capturing long-range contextual dependencies, rendering them particularly well-suited for semantic segmentation tasks. Representative works such as SegFormer \[33\] and Vision Transformer (ViT) \[34\] ...

... Implementation details. The proposed method in this paper is based on the Transformer \[31\] architecture, utilizing the Vision Transformer ViT-B/16 \[34\] pre-trained on ImageNet \[59\] as the encoder and two Transformer layers as the decoder, with the input image resolution set to 512 × 512. Following common settings \[51\], the initial learning rate for both datasets is set to 1 × 10 −3 , and the model is trained using the SGD optimizer with a momentum of 0.9. ...

[Adaptive multi-scale framework for incremental semantic segmentation](publication/396908885_Adaptive_multi-scale_framework_for_incremental_semantic_segmentation)

Article

Full-text available

*   Oct 2025
*   PATTERN ANAL APPL

Class-incremental semantic segmentation (CISS) aims to progressively learn new categories or adapt to novel environments for semantic segmentation tasks without requiring full model retraining, while ensuring that segmentation performance on previously learned classes is preserved. Most existing class-incremental semantic segmentation methods mitigate the issues of catastrophic forgetting and background shift through strategies such as pseudo-labeling and knowledge distillation. Although existing methods have achieved certain success, they still have some limitations: (1) they ignore contextual relationships between tasks, resulting in a lack of holistic learning; (2) knowledge from the old model is directly transferred to the new model, even though not all of it is beneficial for the new model. This paper proposes a novel Adaptive Multi-scale Incremental Semantic Segmentation (AMIS) framework that incorporates a Global Attention Block (GAB), designed to capture contextual information across different tasks and address these challenges. In addition, this work employs an Adaptive Multi-scale Distillation (AMD) module to perform multi-scale pooling and fusion on features extracted by the decoder, enabling the model to adaptively focus on informative representations. Moreover, a Background Compensation Strategy (BCS) is applied to enhance the model’s ability to distinguish ambiguous boundaries between background and target classes. Extensive experiments on the Pascal VOC and ADE20K datasets demonstrate that the proposed method effectively mitigates catastrophic forgetting and background shift, outperforming state-of-the-art approaches in most scenarios. The code is available at https://github.com/ZXCV-7/AMIS.

[View](publication/396908885_Adaptive_multi-scale_framework_for_incremental_semantic_segmentation)

Show abstract

... However, the system's current limitations suggest that it may be less suitable for applications requiring complex temporal modeling or hierarchical pattern recognition (Tkemaladze, 2025n). In domains such as natural language processing or video analysis, where multi-scale temporal dependencies are essential, more sophisticated architectures like LSTM networks or transformers may remain preferable despite their computational intensity (Vaswani et al., 2017). ...

... Attention mechanisms, as popularized in transformer architectures, could enable the system to dynamically focus computational resources on the most informative patterns and time scales (Vaswani et al., 2017). This capability would implement a computational analog of the attentional modulation observed throughout biological perceptual hierarchies (Desimone & Duncan, 1995). ...

[Bayesian Order in Ze](publication/396903903_Bayesian_Order_in_Ze)

Article

Full-text available

*   Oct 2025

*   [![Jaba Tkemaladze](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Jaba Tkemaladze](https://www.researchgate.net/profile/Jaba-Tkemaladze-3)

This article presents the Ze artificial life system, a novel bio-inspired architecture for predictive processing in infinite data streams under severe memory constraints. The system implements Bayesian probability updating through a mechanism of dynamic chronotropic frequency analysis, demonstrating remarkable computational efficiency and biological plausibility. Unlike traditional approaches such as LSTM networks and Markov models, Ze processes information through parallel beginning and inverse processors, enabling complementary pattern discovery while maintaining sublinear memory complexity. The core algorithm exhibits distinctive probability dynamics characterized by an initial match probability of 0.5 with exponential decay to 0.00001 as counter diversity increases, achieving 78-92% prediction accuracy for stable data flows. Experimental results using synthetic datasets (1,048,576 binary sequences) confirm 37-42% operational savings compared to conventional methods, rapid adaptation to changing stream characteristics within 2-3 seconds, and robust noise tolerance up to 15% input distortion. The Go implementation processes 1.2 million operations per second with 850 nanosecond latency while maintaining memory usage of 12.8 bytes per counter. The system's architecture shows strong neurobiological correlations with predictive coding principles and synaptic plasticity mechanisms, providing both a practical solution for resource-constrained environments and a computational model of Bayesian inference in neural systems. Future development pathways include extension to non-binary data streams, integration with hierarchical Bayesian models, and hardware acceleration through memristor-based implementations.

[View](publication/396903903_Bayesian_Order_in_Ze)

Show abstract

... Transformer-based approaches were initially introduced for Natural Language Processing \[90\]. Such methods can easily be adapted for time series classification tasks, and in this paper we propose SiT (Signal Transformer), an extension of a recent computer vision transformer approach \[32\]. ...

... After the embedding step, the input is mapped to a D dimensional space (we use D = 256 in the rest of the paper) that serves as input to an encoder. For SiT, we use an encoder originally proposed for computer vision tasks \[90\] that consists of multiple blocks. Each block has an alternating multi-headed self-attention block and a feed-forward layer, both preceded by a normalization step and a residual connection. ...

[MSAD: A deep dive into model selection for time series anomaly detection](publication/396902601_MSAD_A_deep_dive_into_model_selection_for_time_series_anomaly_detection)

Article

Full-text available

*   Oct 2025
*   VLDB J

Anomaly detection is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. Despite increasing academic interest and the large number of methods proposed in the literature, recent benchmarks and evaluation studies demonstrated that no overall best anomaly detection methods exist when applied to very heterogeneous time series datasets. Therefore, the only scalable and viable solution to solve anomaly detection over very different time series collected from diverse domains is to propose a model selection method that will select, based on time series characteristics, the best anomaly detection methods to run. Existing AutoML solutions are, unfortunately, not directly applicable to time series anomaly detection, and no evaluation of time series-based approaches for model selection exists. Towards that direction, this paper studies the performance of time series classification methods used as model selection for anomaly detection. In total, we evaluate 234 model configurations derived from 16 base classifiers across more than 1980 time series, and we propose the first extensive experimental evaluation of time series classification as model selection for anomaly detection. Our results demonstrate that model selection methods outperform every single anomaly detection method while being in the same order of magnitude regarding execution time. This evaluation is the first step to demonstrate the accuracy and efficiency of time series classification algorithms for anomaly detection, and represents a strong baseline that can then be used to guide the model selection step in general AutoML pipelines.

[View](publication/396902601_MSAD_A_deep_dive_into_model_selection_for_time_series_anomaly_detection)

Show abstract

... Schematic of common AI algorithms in pathology. (a) U-Net, \[112\] (b) CNN, \[113\] (c) GAN, \[114\] (d) Transformer, \[115\] (e) ResNet. \[116\] ...

[Next‐Generation Pathology Unveiled: AI‐Enhanced Label‐Free Microimaging](publication/396910453_Next-Generation_Pathology_Unveiled_AI-Enhanced_Label-Free_Microimaging)

Article

Full-text available

*   Oct 2025
*   [LASER PHOTONICS REV](journal/Laser-Photonics-Reviews-1863-8899)

Pathological diagnosis is integral to disease detection, therapeutic decision‐making, and prognosis. Despite advances in digital pathology, current methods depend on chemically stained slides, which are labor‐intensive and time‐consuming. Label‐free microimaging techniques offer a promising alternative, capturing intrinsic physiological and structural information from biological tissues without chemical labeling or complex preparation. These modalities provide high‐resolution, nondestructive imaging of tissue architecture and pathology‐relevant biomarkers. However, the complexity of the instrumentation and difficulty of interpreting rich, multidimensional data pose significant barriers to clinical deployment. To address these challenges, artificial intelligence (AI)‐assisted methods, particularly deep learning, are being developed to reduce manual workloads and streamline pathology workflows. This review summarizes recent advancements in integrating label‐free optical imaging with AI in digital pathology. We highlight the role of deep learning models in enhancing image quality and automating pathological analysis. In addition, we discuss unresolved issues, such as limited model generalizability and clinical validation gaps, while suggesting future directions, including hardware innovations and foundation AI models. The integration of AI and label‐free microimaging is expected to advance digital pathology toward more intelligent, efficient, and precise diagnostics.

[View](publication/396910453_Next-Generation_Pathology_Unveiled_AI-Enhanced_Label-Free_Microimaging)

Show abstract

... The architecture of the Audio Transformer \[39\] in Fig. 3 begins with a sequential convolutional structure designed to extract preliminary characteristics from the audio data. The architectural design subsequently incorporates a Transformer block \[48\], a widely utilized component for processing sequential data. This section employs a Multi-Head Self Attention \[49\] layer to handle the aforementioned characteristics. ...

[LightAudioCNN: a novel deep neural network for audio-based parkinson’s disease recognition and subtype differentiation](publication/396906440_LightAudioCNN_a_novel_deep_neural_network_for_audio-based_parkinson's_disease_recognition_and_subtype_differentiation)

Article

Full-text available

*   Oct 2025
*   PATTERN ANAL APPL

This study introduces a Deep Neural Network architecture called LightAudioCNN. Its main purpose is to examine cord vibration patterns to improve the diagnosis of Parkinsons’ disease (PD) and differentiate it from similar conditions. LightAudioCNN represents a step in developing more objective and precise diagnostic tools, especially crucial in the early stages of PD, unlike the conventional symptom-based methods known for their arbitrary and unreliable nature. By analyzing vowel sounds (“a” and “i”) from a dataset of 83 participants, this study evaluates LightAudioCNN’s effectiveness while ensuring the reliability of its outcomes using a patient separation method. LightAudioCNN demonstrates high diagnostic accuracy and efficiency, achieving an Area Under the Curve (AUC) score of 0.99 in binary classification tasks and 0.96 in multiclass classification tasks with corresponding accuracy rates of 95% and 81%. These results were obtained through comparisons with Deep Neural Networks trained on Mel Spectrograms and contemporary transformer models processing Mel spectrograms or raw audio data. Additionally, the application of LightAudioCNN to the Italian Parkinson Speech dataset further substantiates its high diagnostic capability. On this dataset, LightAudioCNN achieved a mean accuracy of 97.69%, a precision of 97.88%, and an AUC score of 0.9873, illustrating its ability to capture complex speech patterns associated with Parkinson’s disease. The model’s performance was in line with the other deep learning models. Furthermore, the study highlights the versatility of LightAudioCNN beyond Parkinsons’ disease by proving its superiority in identifying COVID-19 by analyzing breath patterns and cough sounds. In this comparison, LightAudioCNN surpasses deep learning and traditional machine learning models by achieving a mean accuracy of 78.81% in the same scenarios. This proves the model’s potential for quickly and accurately diagnosing COVID-19, demonstrating its relevance across conditions. The model also has a small footprint of about 3.1 M parameters, which is about 7 times less than standard computer vision architectures such as ResNet50, allowing the integration of this technology locally into smartphone applications with the aim of managing and treating not just Parkinson’s’ Disease but also emerging health threats, like COVID-19.

[View](publication/396906440_LightAudioCNN_a_novel_deep_neural_network_for_audio-based_parkinson's_disease_recognition_and_subtype_differentiation)

Show abstract

... The transformer model can more accurately capture the complex nonlinear relationships between parameters and objective functions through deep learning. In particular, when dealing with high-dimensional parameter spaces, the self-attention \[51\]. ...

[Multi-objective optimization of aluminum alloy control arm manufacturing process parameters based on the Transformer-NSGA-II model](publication/396903379_Multi-objective_optimization_of_aluminum_alloy_control_arm_manufacturing_process_parameters_based_on_the_Transformer-NSGA-II_model)

Article

Full-text available

*   Oct 2025
*   INT J ADV MANUF TECH

To address the issues of unstable forming quality in aluminum alloy control arm forgings and low die service life while minimizing forging costs, this study studies the influence of process parameters (blank temperature, die temperature, upper die velocity, and friction coefficient) on target parameters (maximum forming force, maximum die wear depth, and maximum damage value of forging) through numerical simulation. A multi-objective process parameter optimization approach based on the Transformer-NSGA-II model is proposed. First, the Box-Behnken experimental design was adopted to obtain data from different process parameter combinations via finite element analysis (FEA). Then, the transformer model with self-attention was utilized to establish the regression model between the process parameters and target parameters. Finally, the NSGA-II algorithm was applied to identify the optimal parameter combination. To validate the efficiency of the proposed method, a comparative analysis of variance was conducted against commonly used optimization methods. The top 10 solutions in the Pareto solution set of each method were selected and ranked via the entropy-weighted TOPSIS method. The results demonstrate that the proposed model has a lower variance and provides superior process parameter combinations. The FEA verified the optimization results, showing a reduction in the maximum forming force from 20,251 to 18,829 KN, a decrease of 7.02%; the maximum die wear depth decreased from 0.0905 to 0.0784 μm, a reduction of 13.37%; and the maximum damage value of forging decreased from 0.413 to 0.322, a decline of 29.47%. Furthermore, actual trial production confirmed that the forgings produced under the optimal parameter combination meet the quality requirements. This approach achieves synergistic optimization of the forming efficiency, forming quality and die life, demonstrating significant engineering value in enhancing the intelligence and productivity of aluminum alloy control arm forging processes. It also provides a practical process parameter optimization model for digital manufacturing and intelligent manufacturing.

[View](publication/396903379_Multi-objective_optimization_of_aluminum_alloy_control_arm_manufacturing_process_parameters_based_on_the_Transformer-NSGA-II_model)

Show abstract

... The first model with classical data and classical algorithms (C-C) represents the standard AI approach. Deep learning architectures such as Transformers and U-Net have demonstrated exceptional performance in tasks such as image recognition, natural language processing, and scientific computing (Ronneberger et al. 2015;Vaswani et al. 2017 State Key Laboratory of Digital Sensing and Processing IC Technology, Nanjing 211189, Jiangsu, China models are deepened, they introduce redundant parameters, increased training costs, and the risk of overfitting (Santos and Papa 2022). For fully quantum (Q-Q) models on faulttolerant quantum computers, current limitations in qubit coherence, noise, and scalability render them a theoretical vision rather than a near-term reality. ...

[HiQC: hybrid quantum classical deep learning with parameter efficient quantum modules](publication/396903091_HiQC_hybrid_quantum_classical_deep_learning_with_parameter_efficient_quantum_modules)

Article

Full-text available

*   Oct 2025

As deep learning models grow in complexity, they suffer from increasing redundant parameters and high computational costs. Quantum computing offers a new computational paradigm; however, fully quantum AI remains impractical due to current quantum hardware constraints. This raises a fundamental problem: Can quantum AI serve as an efficient module within classical AI architectures to enhance performance while reducing computational overhead? In this work, we explore a hybrid quantum-classical AI framework that embeds parameter-efficient quantum computing modules within classical deep learning models. Specifically, we incorporate a quantum bottleneck module into a U-Net architecture, called QB-Net. Experiments and quantitative analysis in real-world datasets for medical and natural image processing demonstrate that QB-Net reduces up to 36.3×\\times parameters in the bottleneck versus its classical counterpart, while achieving performance comparable to classical U-Net in all metrics considered. Our results demonstrate that quantum AI can serve as a plug-and-play enhancement, leveraging quantum advantages to optimize classical architectures without altering their fundamental structure.

[View](publication/396903091_HiQC_hybrid_quantum_classical_deep_learning_with_parameter_efficient_quantum_modules)

Show abstract

... Self-attention mechanisms have become a fundamental concept in the field of deep learning, especially within the domains of natural language processing (NLP) and computer vision (CV) (Vaswani et al. 2017). These mechanisms markedly improve model performance by capturing the interdependencies between sequences during data processing. ...

[Classifying encrypted traffic using quad-directional convolution on pulse sequences](publication/396902457_Classifying_encrypted_traffic_using_quad-directional_convolution_on_pulse_sequences)

Article

Full-text available

*   Oct 2025

With the growing concerns for user privacy and communication security, the volume of encrypted traffic has surged, making accurate classification of encrypted traffic crucial. Current mainstream classification methods primarily rely on handcrafted features and machine learning techniques. However, these features often depend on expert knowledge and require substantial human resources. Additionally, using simple features can lead to inadequate feature extraction. To address these challenges, we have developed the Pulse-Driven Quad-Directional Temporal Convolutional Network. A core innovation of this model is transforming one-dimensional packet length sequences into two-dimensional pulse sequences, significantly enhancing the feature representation capability and classification accuracy of the model. Furthermore, the model employs forward, backward, middle-outward, and both-ends-inward temporal convolutions to effectively extract global features. Coupled with the self-attention mechanism, the model deeply explores the dependencies among features, greatly enhancing classification accuracy and robustness. On the public CESNET-TLS22 dataset, this method achieved an F1 score of 99.24% in the classification of five application services and an average F1 score of 98.74% in the 20-application classification test. Additionally, on the UNB ISCX dataset, tests on VPN and Tor traffic also demonstrated outstanding performance, with F1 scores exceeding 98%. These results highlight the precision and effectiveness of this method across various application scenarios.

[View](publication/396902457_Classifying_encrypted_traffic_using_quad-directional_convolution_on_pulse_sequences)

Show abstract

... Considering a network per timestep would be inefficient because the total number of parameters to learn would be enormous. To tackle this, a single deep neural network is designed for an image-to-image or graph-to-graph mapping and a sinusoidal position embedding (Vaswani et al., 2023) is added to account for the timestep. ...

[A Deep Generative Model for the Simulation of Discrete Karst Networks](publication/396901959_A_Deep_Generative_Model_for_the_Simulation_of_Discrete_Karst_Networks)

Article

Full-text available

*   Oct 2025

The simulation of discrete karst networks presents a significant challenge due to the complexity of the physicochemical processes at their origin, occurring within various geological and hydrogeological contexts over extended periods. This complex interplay leads to a wide variety of karst network patterns, each intricately linked to specific hydrogeological conditions. We explore a novel approach that represents karst networks as graphs and applies graph generative models (deep learning techniques) to capture the intricate nature of karst environments. In this representation, nodes retain spatial information and properties, while edges signify connections between nodes. Our generative process consists of two main steps. First, we utilize graph recurrent neural networks (GraphRNN) to learn the topological distribution of karst networks. GraphRNN decomposes the graph simulation into a sequential generation of nodes and edges, informed by previously generated structures. Second, we employ denoising diffusion probabilistic models on graphs (G‐DDPM) to learn node features (spatial coordinates and other properties). G‐DDPMs enable the generation of nodes features on the graphs produced by the GraphRNN that adhere to the learned statistical properties by sampling from the derived probability distribution, ensuring that the generated graphs are realistic and capture the essential features of the original data. We test our approach using real‐world karst networks and compare generated subgraphs with actual subgraphs from the database, by using geometry and topology metrics. Our methodology allows stochastic simulation of discrete karst networks across various types of formations, a useful tool for studying the behavior of physical processes such as flow and transport.

[View](publication/396901959_A_Deep_Generative_Model_for_the_Simulation_of_Discrete_Karst_Networks)

Show abstract

... Researchers and companies envision empathetic conversational agents that can comfort, persuade, or tutor with emotional attunement \[8,9\]. Transformer-based LLMs such as GPT-3 and GPT-4 \[10\]\[11\]\[12\] have demonstrated uncanny fluency and even a veneer of empathy in controlled settings. Fine-tuning with human feedback (RLHF) further imbues these models with a consistent friendly persona and adherence to social norms \[13,14\]. ...

[Algorithmic Affective Blunting: Quantifying the Collapse Curve of Interpretative Failure in Large Language Models](publication/396899394_Algorithmic_Affective_Blunting_Quantifying_the_Collapse_Curve_of_Interpretative_Failure_in_Large_Language_Models)

Preprint

Full-text available

*   Oct 2025

We report a robust, dose-dependent degradation of affective interpretation in large language models (LLMs) under semantic stress, which we term Algorithmic Affective Blunting (AAB). Using a Hierarchical Hermeneutic Stress Protocol (HHSP) and an ordinal Affective Degradation Index (ADI; 0–3), we chart a monotonic Collapse Curve. In this revision, we disentangle Phase 3 perturbations into Noise-only and Persona-only subconditions with length-matching, add an empirically grounded simulated Base vs. Instruct causal probe (same architecture/size/decoding; no new API calls) to test the hypothesized alignment–brittleness relationship, and introduce a computational proxy for ADI to enhance objectivity and scalability. We clarify that the “affective integrator” is a conceptual device rather than a mechanistic claim. The study complements recent theoretical frameworks on affective selfhood and sovereignty by providing an empirical benchmark for interpretative degradation and emotional robustness in LLMs. The findings are directly applicable to affect-rich AI deployments such as conversational and counseling systems.

[View](publication/396899394_Algorithmic_Affective_Blunting_Quantifying_the_Collapse_Curve_of_Interpretative_Failure_in_Large_Language_Models)

Show abstract

... An LLM learns associations from scratch during its training phase-over billions of training runs, its attention network slowly encodes the structure of the language it sees as numbers (called "weights") within its neural network. Text is split into tokens, which are words like "love" or "are", affixes, like "dis" or "ised", and punctuation, like "?" .Transfer learning is a technique which relies on a method which allows any given sequence to be re-ordered by weighting the importance of elements of the input in an N-dimensional space \[22\]. The result is what is now referred to as a foundation model \[23\]. ...

[AI in Money Matters](publication/396896686_AI_in_Money_Matters)

Article

*   May 2025

In November 2022, Europe and the world by and large were stunned by the birth of a new large language model : ChatGPT. Ever since then, both academic and populist discussions have taken place in various public spheres such as LinkedIn and X(formerly known as Twitter) with the view to both understand the tool and its benefits for the society. The views of real actors in professional spaces, especially in regulated industries such as finance and law ha ve been largely missing. We aim to begin to close this gap by presenting results from an empirical investigation conducted through interviews with professional actors in the Fintech industry. The paper asks the question, how and to what extent are large language models in general and ChatGPT in particular being adopted and used in the Fintech industry? The results show that while the fintech experts we spoke with see a potential in using large language models in the future, a lot of questions marks remain concerning how they are policed and therefore might be adopted in a regulated industry such as Fintech. This paper aims to add to the existing academic discussing around large language models, with a contribution to our understanding of professional viewpoints.

[View](publication/396896686_AI_in_Money_Matters)

Show abstract

... Subsequently, they utilized an HNN composed of graph TCN (GTCN) and graph attention network (GAN), specifically designed for this type of data, to predict spatiotemporal wind speed data. Transformer (Vaswani et al. 2017) is a deep learning model constructed with an encoder-decoder architecture based on multihead self-attention mechanisms. This model comprises multiple selfattention mechanisms assembled into multihead self-attention mechanisms, which then form the encoder and decoder modules. ...

[A Novel Interpretable Deep Learning‐Based Wind Speed and Power Generation Forecasting Using Multiscale Attention and Post Hoc Feature Importance Mechanism](publication/396894848_A_Novel_Interpretable_Deep_Learning-Based_Wind_Speed_and_Power_Generation_Forecasting_Using_Multiscale_Attention_and_Post_Hoc_Feature_Importance_Mechanism)

Article

Full-text available

*   Oct 2025
*   [J FORECASTING](journal/Journal-of-Forecasting-1099-131X)

Accurate and efficient wind speed forecasting can enhance the scheduling of wind farms and ensure the stable operation of power grids. However, the inherent stochastic variability and complex fluctuation patterns of wind speed sequences increase the difficulty of forecasting, and existing deep learning‐based forecasting methods struggle to provide interpretable results. This study proposes an interpretable wind speed forecasting method based on deep learning. This method integrates two‐stage decomposition, time series embedding, a dual‐channel hybrid neural network, advanced attention mechanisms, and meta‐heuristic algorithms to achieve precise and efficient wind speed predictions. In addition, this study introduces a model‐agnostic post hoc feature importance ranking method for interpretability, which enhances the interpretability of the forecasting model by processing test data to output feature importance rankings. After wind speed predictions are completed, this research incorporates real wind turbine data to perform wind power conversion for enhancing its practical value. The designed ablation experiments and multiple comparative experiments in this study validate the comprehensiveness and advancement of the model. The interpretability results and wind power conversion outcomes also provide additional analytical perspectives for related decision‐making processes.

[View](publication/396894848_A_Novel_Interpretable_Deep_Learning-Based_Wind_Speed_and_Power_Generation_Forecasting_Using_Multiscale_Attention_and_Post_Hoc_Feature_Importance_Mechanism)

Show abstract

[The Evolution and Latest Trends of AI Technology](publication/396909420_The_Evolution_and_Latest_Trends_of_AI_Technology)

Chapter

*   Oct 2025

Generative artificial intelligence (GenAI), exemplified by large language models, has brought disruptive impacts across diverse sectors. This current wave of AI advancement distinguishes itself from historical information revolutions, by extending its influence to the core fabric of society, including education and culture.

[View](publication/396909420_The_Evolution_and_Latest_Trends_of_AI_Technology)

Show abstract

[EnSSNet: An Advanced Ensemble Self-Supervised Learning Framework with Mini Batch-Graph Convolutional Network and Deep Curriculum Learning for Robust PolSAR Image Classification](publication/396909048_EnSSNet_An_Advanced_Ensemble_Self-Supervised_Learning_Framework_with_Mini_Batch-Graph_Convolutional_Network_and_Deep_Curriculum_Learning_for_Robust_PolSAR_Image_Classification)

Article

*   Oct 2025
*   KNOWL-BASED SYST

[View](publication/396909048_EnSSNet_An_Advanced_Ensemble_Self-Supervised_Learning_Framework_with_Mini_Batch-Graph_Convolutional_Network_and_Deep_Curriculum_Learning_for_Robust_PolSAR_Image_Classification)

[Spatio-Temporal Adaptive Fusion Transformer for Next POI Recommendation](publication/396908934_Spatio-Temporal_Adaptive_Fusion_Transformer_for_Next_POI_Recommendation)

Article

*   Oct 2025

[View](publication/396908934_Spatio-Temporal_Adaptive_Fusion_Transformer_for_Next_POI_Recommendation)

[Hybrid transformer DDPG framework for solar radiation forecasting and battery energy storage optimization in a PV-powered microgrid](publication/396908385_Hybrid_transformer_DDPG_framework_for_solar_radiation_forecasting_and_battery_energy_storage_optimization_in_a_PV-powered_microgrid)

Article

*   Oct 2025

This study proposes a hybrid framework integrating a Transformer-based deep learning model for solar radiation forecasting with a Deep Deterministic Policy Gradient (DDPG) reinforcement learning agent for optimizing battery energy storage system (BESS) management in a photovoltaic (PV)-powered microgrid. Leveraging historical meteorological data from the National Solar Radiation Database (NSRDB) India dataset, the Transformer model predicts Global Horizontal Irradiance (GHI), which is used to estimate PV power output. This predicted PV power drives the DDPG agent, trained over 1000 episodes using MATLAB, to dynamically manage BESS charge/discharge rates. Compared to a rule-based baseline controller, the hybrid approach achieves superior performance, with an energy efficiency of 98.5% (vs. 85.2% for the baseline) and a 64% reduction in unmet demand over a 1749-hour simulation, alongside greater battery utilization. This work demonstrates the effectiveness of integrating advanced forecasting with adaptive control, offering a scalable solution for enhancing renewable energy systems in microgrids.

[View](publication/396908385_Hybrid_transformer_DDPG_framework_for_solar_radiation_forecasting_and_battery_energy_storage_optimization_in_a_PV-powered_microgrid)

Show abstract

[Image-text relationship-based feature interaction networks for multimodal aspect-based sentiment analysis](publication/396908206_Image-text_relationship-based_feature_interaction_networks_for_multimodal_aspect-based_sentiment_analysis)

Article

Full-text available

*   Oct 2025
*   PATTERN ANAL APPL

The multimodal aspect-based sentiment analysis (MABSA) task aims to determine the sentiment polarity associated with each specific aspect term mentioned in the text by integrating multimodal information, such as textual and visual data. This task is of significant importance for deeper comprehension of user sentiments and opinions. However, existing research suffers from the following limitation: (1) Although a correspondence exists between visual regions and words, current methods struggle to accurately learn the correct region-word alignment. (2) 2D images contain numerous features irrelevant to sentiment analysis, and existing methods lack an effective denoising mechanism. To tackle the mentioned issues, this paper provides a novel image-text relationship-based feature interaction network (ITRIN). Specifically, this paper introduces a cross-modal alignment correction module to learn the correspondence between region-level visual features and textual tokens and employs an adaptive gating mechanism to mitigate the adverse effects of misaligned region-word pairs. Furthermore, by incorporating global contextual information, we design a gating mechanism based on the probability scores of the image-text relationship (BRGate) in the final feature fusion layer, which enables deep filtering and effective fusion of multimodal features. The proposed model, ITRIN, achieves state-of-the-art performance on two benchmark datasets, Twitter-2015 and Twitter-2017. Extensive ablation studies further validate its superiority and effectiveness. The source code is publicly released at https://github.com/MHTransKanba/ITRIN.

[View](publication/396908206_Image-text_relationship-based_feature_interaction_networks_for_multimodal_aspect-based_sentiment_analysis)

Show abstract

[PIDA-Net: A Prior Image-Guided Deformable Attention Network for High-Quality 4D-CBCT Reconstruction](publication/396907674_PIDA-Net_A_Prior_Image-Guided_Deformable_Attention_Network_for_High-Quality_4D-CBCT_Reconstruction)

Article

*   Oct 2025
*   APPL RADIAT ISOTOPES

[View](publication/396907674_PIDA-Net_A_Prior_Image-Guided_Deformable_Attention_Network_for_High-Quality_4D-CBCT_Reconstruction)

[Convolutional and Graph Neural Network Framework for Predicting Critical Impact Velocity in Heterogeneous PBX‐9501](publication/396906599_Convolutional_and_Graph_Neural_Network_Framework_for_Predicting_Critical_Impact_Velocity_in_Heterogeneous_PBX-9501)

Article

*   Oct 2025
*   PROPELL EXPLOS PYROT

Heterogeneous energetic materials (HEM) can involve structural defects such as randomly distributed pores of varying size and shape. The unique arrangements of these defects cause initiation metrics such as pressure, temperature, and particle velocity to vary on a sample‐to‐sample basis. Current methods for predicting initiation rely on experiments and computational models. However, accounting for each possible pore configuration requires an extensive number of experiments and computational simulations, making them unfeasible for this problem. Machine learning (ML) offers an attractive approach to overcome these challenges. Towards this goal, this work introduces an ML framework involving a convolutional neural network (CNN) and a graph neural network (GNN) for predicting critical velocities in PBX‐9501 samples with multiple pores of varying quantity, size, and spatial distribution. The performance of both models was evaluated across two types of pore arrangements: Cartesian grids and rotated configurations. The comparative evaluation showed that the GNN outperformed the CNN in Cartesian grid pore configurations, achieving a lower average error of compared to the CNN of . Conversely, for rotated pore arrangements, the CNN achieved better accuracy of than the GNN of . Despite these differences, both models consistently achieved average prediction errors below , demonstrating strong overall performance across different pore configurations. Ultimately, this work advances the development of ML‐driven models capable of rapidly and accurately predicting how complex pore structures influence shock sensitivity in HEM(s).

[View](publication/396906599_Convolutional_and_Graph_Neural_Network_Framework_for_Predicting_Critical_Impact_Velocity_in_Heterogeneous_PBX-9501)

Show abstract

[A frequency loss function based dynamic convolutional transformer model with data denoising for short-term wind speed forecasting](publication/396905934_A_frequency_loss_function_based_dynamic_convolutional_transformer_model_with_data_denoising_for_short-term_wind_speed_forecasting)

Article

*   Oct 2025
*   EXPERT SYST APPL

[View](publication/396905934_A_frequency_loss_function_based_dynamic_convolutional_transformer_model_with_data_denoising_for_short-term_wind_speed_forecasting)

[A multi-resolution spatiotemporal semantic learning approach for origin-destination demand prediction in urban rail transit network](publication/396905610_A_multi-resolution_spatiotemporal_semantic_learning_approach_for_origin-destination_demand_prediction_in_urban_rail_transit_network)

Article

*   Oct 2025
*   COMPUT IND ENG

[View](publication/396905610_A_multi-resolution_spatiotemporal_semantic_learning_approach_for_origin-destination_demand_prediction_in_urban_rail_transit_network)

[A method for assessing the odor intensity of polypropylene materials with an electronic nose to simulate a panel of olfactory sensory](publication/396905373_A_method_for_assessing_the_odor_intensity_of_polypropylene_materials_with_an_electronic_nose_to_simulate_a_panel_of_olfactory_sensory)

Article

*   Oct 2025
*   CHEM ENG J

[View](publication/396905373_A_method_for_assessing_the_odor_intensity_of_polypropylene_materials_with_an_electronic_nose_to_simulate_a_panel_of_olfactory_sensory)

[Position-matched surface roughness prediction of thin-walled parts in milling using the spatial transformer to spindle vibration signals](publication/396905019_Position-matched_surface_roughness_prediction_of_thin-walled_parts_in_milling_using_the_spatial_transformer_to_spindle_vibration_signals)

Article

*   Oct 2025
*   MEASUREMENT

[View](publication/396905019_Position-matched_surface_roughness_prediction_of_thin-walled_parts_in_milling_using_the_spatial_transformer_to_spindle_vibration_signals)

[MHFFNet: multi-category hybrid feature fusion network for few-shot anomaly detection](publication/396904282_MHFFNet_multi-category_hybrid_feature_fusion_network_for_few-shot_anomaly_detection)

Article

*   Oct 2025
*   NEUROCOMPUTING

[View](publication/396904282_MHFFNet_multi-category_hybrid_feature_fusion_network_for_few-shot_anomaly_detection)

[A Hierarchical Transformer and Graph Neural Network Model for High-Accuracy Watershed Nitrate Prediction](publication/396904236_A_Hierarchical_Transformer_and_Graph_Neural_Network_Model_for_High-Accuracy_Watershed_Nitrate_Prediction)

Article

*   Oct 2025

[View](publication/396904236_A_Hierarchical_Transformer_and_Graph_Neural_Network_Model_for_High-Accuracy_Watershed_Nitrate_Prediction)

[Neural Networks in the Baikal-GVD Experiment: Selection of Neutrino Events and Neutrino Energy Reconstruction](publication/396903492_Neural_Networks_in_the_Baikal-GVD_Experiment_Selection_of_Neutrino_Events_and_Neutrino_Energy_Reconstruction)

Article

*   Oct 2025
*   PHYS PART NUCLEI+

[View](publication/396903492_Neural_Networks_in_the_Baikal-GVD_Experiment_Selection_of_Neutrino_Events_and_Neutrino_Energy_Reconstruction)

[Deep Learning and Attention-Based Methods for Human Activity Recognition and Anticipation: A Comprehensive Review](publication/396903164_Deep_Learning_and_Attention-Based_Methods_for_Human_Activity_Recognition_and_Anticipation_A_Comprehensive_Review)

Article

Full-text available

*   Oct 2025

In recent years, there has been a significant increase in research focused on Human Activity Analysis (HAA). This field has progressed from basic activity recognition tasks to addressing more challenging ones, such as predicting future human actions based on partially observed videos and even predicting actions before they happen. The evolution of HAA has been driven by recent advancements in attention-based models like Transformers, along with a wide range of applications from security surveillance to advanced monitoring systems, behaviour analysis, and more. A comprehensive review of HAA literature from 2017 to 2025, with a novel taxonomy emphasising activity recognition, prediction, and anticipation, is presented. We critically review and examine recognition methods from trimmed and untrimmed videos, context-aware and trajectory-based prediction, and short-term and long-term anticipation. Through a comprehensive analysis, we review and evaluate key aspects of this domain, including attention-based contextual comprehension, temporal dynamics modelling, and multi-model fusion methods. Furthermore, we critically examine and assess the public datasets utilised in driving this research forward, pinpointing limitations and primary challenges within this domain. Finally, the paper provides a summary of recent developments in HAA and suggests future directions, with the hope that it will serve as a valuable reference for researchers in the field.

[View](publication/396903164_Deep_Learning_and_Attention-Based_Methods_for_Human_Activity_Recognition_and_Anticipation_A_Comprehensive_Review)

Show abstract

[A Dual-dimensional Parallel Neural Network Integrating Multi-scale and Frequency-Domain Features for Aircraft Engine Life Prediction](publication/396903103_A_Dual-dimensional_Parallel_Neural_Network_Integrating_Multi-scale_and_Frequency-Domain_Features_for_Aircraft_Engine_Life_Prediction)

Article

*   Oct 2025
*   ARAB J SCI ENG

Continuous monitoring and preventive maintenance based on the prognostics and health management (PHM) technology for engines can ensure the flight safety of aircrafts. Wherein, the remaining useful life (RUL) serves as a critical metric to assess the equipment reliability and determine the maintenance schedule. The increase in data volume makes the data-driven model have richer sample sets, and is helpful to enhance the data fitting ability and prediction accuracy. However, the following issues still exist in the data-driven methods of remaining useful life prediction: (i) Single-dimensional and single-scale approaches are insufficient for comprehensive feature extraction. (ii) Extracting features solely from the time-domain overlooks certain degradation information. Hence, a RUL prediction method, which integrates the time–frequency and multi-scale features along the sequence and feature dimensions, respectively, is proposed. The frequency-domain multi-layer perceptrons for time series (FreTS) is utilized to concurrently extract the frequency-domain features along the sequence and feature dimensions. The atrous convolutions with different rates and the selective fusion strategy are used to extract the multi-scale shallow local feature information, and the encoder is adopted to further capture the deep global degradation information within the entire time step. Experiments on the RUL prediction of aircraft engines are conducted based on the C-MAPSS dataset, and the prediction accuracy is studied. The results indicate that the proposed model effectively extracts and fuses the multi-dimensional, multi-scale, and time–frequency features, and has the higher accuracy in the RUL prediction compared to all the latest methods.

[View](publication/396903103_A_Dual-dimensional_Parallel_Neural_Network_Integrating_Multi-scale_and_Frequency-Domain_Features_for_Aircraft_Engine_Life_Prediction)

Show abstract

[Attention is all you need? When responsiveness short-circuits responsibility](publication/396903072_Attention_is_all_you_need_When_responsiveness_short-circuits_responsibility)

Article

*   Oct 2025
*   AI Soc

[View](publication/396903072_Attention_is_all_you_need_When_responsiveness_short-circuits_responsibility)

[EXPLAINABLE SPAM DETECTION FOR INDIAN LANGUAGES](publication/396900509_EXPLAINABLE_SPAM_DETECTION_FOR_INDIAN_LANGUAGES)

Technical Report

Full-text available

*   Oct 2025

*   [![Praveen siva Kumar](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Praveen siva Kumar](https://www.researchgate.net/profile/Praveen-Kumar-691)

This project presents an Explainable Spam Detection System for Indian Languages, designed to tackle the growing problem of multilingual spam across India’s diverse linguistic landscape. The system leverages IndicBERT and mBERT models for efficient spam detection and language identification, ensuring robust performance across multiple regional languages. To enhance interpretability, LIME (Local Interpretable Model-Agnostic Explanations) is integrated, allowing users and researchers to understand why a message is classified as spam or not. This explainability makes the model more transparent and trustworthy—especially critical in real-world communication systems. The system also includes: Sentiment analysis to detect emotional tone within messages. Language translation using Deep Translate for other Indian languages interpretation. Interactive Flask-based web application for real-time message analysis. SQLite-powered history tracking for previous analyses. By focusing on multilingual adaptability and explainable AI, this research bridges the gap between complex machine learning models and human understanding, contributing to more inclusive and interpretable spam detection systems for the Indian digital ecosystem.

[View](publication/396900509_EXPLAINABLE_SPAM_DETECTION_FOR_INDIAN_LANGUAGES)

Show abstract

[Quantum Neural Networks](publication/396899915_Quantum_Neural_Networks)

Chapter

*   Aug 2025

This chapter presents an in-depth exploration of classical and quantum neural network paradigms, encompassing the fundamental architectures, training methodologies, and theoretical analyses of network performance. This chapter is organized into five sections: Sect. 4.1 reviews the structural and theoretical foundations of classical neural networks; Sect. 4.2 introduces the quantum perceptron model, elucidating its theoretical advantages over classical counterparts; Sect. 4.3 explores quantum neural networks (QNNs), detailing the process by which classical data is encoded into quantum states and processed through parameterized quantum gates, thereby mitigating the challenges posed by large model sizes and high computational costs; Sect. 4.4 delves into the theoretical aspects of QNNs, emphasizing their expressivity, generalization, and trainability; and finally, Sect. 4.5 provides illustrative code implementations using benchmark datasets to demonstrate the practical viability of QNNs.

[View](publication/396899915_Quantum_Neural_Networks)

Show abstract

[Quantum Transformer](publication/396899123_Quantum_Transformer)

Chapter

*   Aug 2025

This chapter provides a comprehensive introduction to the quantum transformer algorithm. In Sect. 5.1 we first described what is the transformer architecture, with detailed explanation about its key subroutines. We also briefly mention the optimization and training. In Sect. 5.2, we provide a guide about designing each quantum subroutine, including quantum self-attention, quantum residual connection with layer norm, and quantum feed-forward neural networks, based on the quantum linear algebra. We further mention various numerical studies on the open-source large language models and provide a detailed discussion about the potential of quantum advantage in Sect. 5.3. Some basic codes are provided in Sect. 5.4. Finally in Sect. 5.5, we provide a bibliographic remark for readers who are interested to explore.

[View](publication/396899123_Quantum_Transformer)

Show abstract

[Efficient Speech Command Recognition Leveraging Spiking Neural Networks and Progressive Time-scaled Curriculum Distillation](publication/396898473_Efficient_Speech_Command_Recognition_Leveraging_Spiking_Neural_Networks_and_Progressive_Time-scaled_Curriculum_Distillation)

Article

*   Oct 2025
*   NEURAL NETWORKS

[View](publication/396898473_Efficient_Speech_Command_Recognition_Leveraging_Spiking_Neural_Networks_and_Progressive_Time-scaled_Curriculum_Distillation)

[AMST-Net: An adaptive multi-scale transformer dual encoder network for skin lesion segmentation](publication/396897729_AMST-Net_An_adaptive_multi-scale_transformer_dual_encoder_network_for_skin_lesion_segmentation)

Article

*   Oct 2025
*   EXPERT SYST APPL

[View](publication/396897729_AMST-Net_An_adaptive_multi-scale_transformer_dual_encoder_network_for_skin_lesion_segmentation)

[Rethinking Employability in the Age of AI: A Critical Analysis of University Strategies for Graduate Success](publication/396897666_Rethinking_Employability_in_the_Age_of_AI_A_Critical_Analysis_of_University_Strategies_for_Graduate_Success)

Chapter

*   Oct 2025

The rapid proliferation of generative artificial intelligence (GenAI) since late 2022 presents a paradigm shift for the global economy, profoundly affecting graduate employability. This chapter explores the prospects of both technological unemployment and job redefinition driven by GenAI’s capabilities. It argues that universities play a pivotal role in preparing the workforce for this transition. A sustainable response requires curriculum reform to embed AI literacy, ethical considerations, and skills for effective human-AI collaboration. Furthermore, pedagogy must prioritise distinctly human capabilities, such as critical thinking, creativity, and empathy. By adopting comprehensive strategies, universities can enhance their role in equipping graduates for an AI-driven future.

[View](publication/396897666_Rethinking_Employability_in_the_Age_of_AI_A_Critical_Analysis_of_University_Strategies_for_Graduate_Success)

Show abstract

[Basics of Quantum Computing](publication/396897343_Basics_of_Quantum_Computing)

Chapter

*   Aug 2025

This chapter introduces the fundamental concepts of quantum computation, such as quantum states, quantum circuits, and quantum measurements, along with key topics in quantum machine learning, including quantum read-in, quantum read-out, and quantum linear algebra. These foundational elements are essential for understanding quantum machine learning algorithms and will be repeatedly referenced throughout the subsequent chapters. This chapter is organized into six sections: Sect. 2.1 introduces quantum bits and their mathematical representations; Sect. 2.2 covers quantum circuits, including quantum gates, quantum channels, and quantum measurements; Sect. 2.3 discusses how to encode classical data into quantum systems and extract classical information from quantum states; Sect. 2.4 explores concepts in quantum linear algebra; Sect. 2.5 provides practical coding exercises to reinforce these concepts; and finally, Sect. 2.6 presents recent advancements in efficient quantum read-in and read-out techniques, as well as developments in quantum linear algebra for further exploration.

[View](publication/396897343_Basics_of_Quantum_Computing)

Show abstract

[STGMAE: A GNSS data-driven pre-training spatiotemporal graph masked autoencoder for agricultural machinery trajectory operation mode identification](publication/396894474_STGMAE_A_GNSS_data-driven_pre-training_spatiotemporal_graph_masked_autoencoder_for_agricultural_machinery_trajectory_operation_mode_identification)

Article

*   Oct 2025

[View](publication/396894474_STGMAE_A_GNSS_data-driven_pre-training_spatiotemporal_graph_masked_autoencoder_for_agricultural_machinery_trajectory_operation_mode_identification)

[Temporal latent diffusion model for machine degradation trend forecasting](publication/396894143_Temporal_latent_diffusion_model_for_machine_degradation_trend_forecasting)

Article

*   Oct 2025
*   KNOWL-BASED SYST

[View](publication/396894143_Temporal_latent_diffusion_model_for_machine_degradation_trend_forecasting)

[Enhancing stock market predictions with multivariate signal decomposition and dynamic feature optimization](publication/396894107_Enhancing_stock_market_predictions_with_multivariate_signal_decomposition_and_dynamic_feature_optimization)

Article

*   Oct 2025
*   N Am J Econ Finance

[View](publication/396894107_Enhancing_stock_market_predictions_with_multivariate_signal_decomposition_and_dynamic_feature_optimization)

[Contrastive Prior Enhances the Performance of Bayesian Neural Network-based Molecular Property Prediction](publication/396893920_Contrastive_Prior_Enhances_the_Performance_of_Bayesian_Neural_Network-based_Molecular_Property_Prediction)

Article

*   Oct 2025
*   EXPERT SYST APPL

[View](publication/396893920_Contrastive_Prior_Enhances_the_Performance_of_Bayesian_Neural_Network-based_Molecular_Property_Prediction)

[Balancing privacy considerations and customization preferences for consumer: LLMs adoption and coordination strategies in supply chains](publication/396893866_Balancing_privacy_considerations_and_customization_preferences_for_consumer_LLMs_adoption_and_coordination_strategies_in_supply_chains)

Article

*   Oct 2025
*   INT J PROD ECON

[View](publication/396893866_Balancing_privacy_considerations_and_customization_preferences_for_consumer_LLMs_adoption_and_coordination_strategies_in_supply_chains)

[(Generative) AI in Financial Economics](publication/396893655_Generative_AI_in_Financial_Economics)

Article

*   Oct 2025
*   J Chin Econ Bus Stud

[View](publication/396893655_Generative_AI_in_Financial_Economics)

[Revisiting the Numerical Feature Embeddings Structure in Neural Network-based Tabular Modelling](publication/396893027_Revisiting_the_Numerical_Feature_Embeddings_Structure_in_Neural_Network-based_Tabular_Modelling)

Article

*   Oct 2025
*   KNOWL-BASED SYST

[View](publication/396893027_Revisiting_the_Numerical_Feature_Embeddings_Structure_in_Neural_Network-based_Tabular_Modelling)

[AttnSeq-PPI: Enhancing protein-protein interaction network prediction using transfer learning-driven hybrid attention](publication/396892876_AttnSeq-PPI_Enhancing_protein-protein_interaction_network_prediction_using_transfer_learning-driven_hybrid_attention)

Article

*   Oct 2025
*   BBA-PROTEINS PROTEOM

[View](publication/396892876_AttnSeq-PPI_Enhancing_protein-protein_interaction_network_prediction_using_transfer_learning-driven_hybrid_attention)

[DiffuseDoc: Document geometric rectification via diffusion model](publication/396892784_DiffuseDoc_Document_geometric_rectification_via_diffusion_model)

Article

*   Oct 2025

[View](publication/396892784_DiffuseDoc_Document_geometric_rectification_via_diffusion_model)

[Self-Supervised EEG Denoising via Dual-Branch Consistency Learning with Masked Reconstruction](publication/396892707_Self-Supervised_EEG_Denoising_via_Dual-Branch_Consistency_Learning_with_Masked_Reconstruction)

Article

*   Oct 2025
*   KNOWL-BASED SYST

[View](publication/396892707_Self-Supervised_EEG_Denoising_via_Dual-Branch_Consistency_Learning_with_Masked_Reconstruction)

[A scale-variable attention transformer for industrial process fault diagnosis](publication/396892633_A_scale-variable_attention_transformer_for_industrial_process_fault_diagnosis)

Article

*   Oct 2025
*   COMPUT CHEM ENG

[View](publication/396892633_A_scale-variable_attention_transformer_for_industrial_process_fault_diagnosis)

[Fusion of Equipment Reliability Data Through Knowledge Graphs](publication/396892120_Fusion_of_Equipment_Reliability_Data_Through_Knowledge_Graphs)

Article

*   Oct 2025
*   RELIAB ENG SYST SAFE

[View](publication/396892120_Fusion_of_Equipment_Reliability_Data_Through_Knowledge_Graphs)

Show more

[Recurrent Neural Network Grammars](publication/305334466_Recurrent_Neural_Network_Grammars)

Conference Paper

Full-text available

*   Feb 2016

*   [Chris Dyer](https://www.researchgate.net/scientific-contributions/Chris-Dyer-70520053)
*   [Adhiguna Kuncoro](https://www.researchgate.net/scientific-contributions/Adhiguna-Kuncoro-2108175573)
*   [![Miguel Ballesteros](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Miguel Ballesteros](https://www.researchgate.net/profile/Miguel-Ballesteros)
*   [Noah A. Smith](https://www.researchgate.net/scientific-contributions/Noah-A-Smith-71025823)

We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.

[View](publication/305334466_Recurrent_Neural_Network_Grammars)

Show abstract

[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](publication/269416998_Empirical_Evaluation_of_Gated_Recurrent_Neural_Networks_on_Sequence_Modeling)

Article

Full-text available

*   Dec 2014

*   [Junyoung Chung](https://www.researchgate.net/scientific-contributions/Junyoung-Chung-2060342625)
*   [![Caglar Gulcehre](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Caglar Gulcehre](https://www.researchgate.net/profile/Caglar-Gulcehre)
*   [Kyunghyun Cho](https://www.researchgate.net/scientific-contributions/Kyunghyun-Cho-69952266)
*   [![Y. Bengio](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Y. Bengio](https://www.researchgate.net/profile/Y-Bengio)

In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.

[View](publication/269416998_Empirical_Evaluation_of_Gated_Recurrent_Neural_Networks_on_Sequence_Modeling)

Show abstract

[Fast and Accurate Shift-Reduce Constituent Parsing](publication/266376373_Fast_and_Accurate_Shift-Reduce_Constituent_Parsing)

Conference Paper

Full-text available

*   Jun 2013

*   [![Muhua Zhu](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Muhua Zhu](https://www.researchgate.net/profile/Muhua-Zhu)
*   [![Yue Zhang](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Yue Zhang](https://www.researchgate.net/profile/Yue-Zhang-168)
*   [![Wenliang Chen](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Wenliang Chen](https://www.researchgate.net/profile/Wenliang-Chen-2)
*   [![Jingbo Zhu](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Jingbo Zhu](https://www.researchgate.net/profile/Jingbo-Zhu-3)

Shift-reduce dependency parsers give comparable accuracies to their chart-based counterparts, yet the best shift-reduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.

[View](publication/266376373_Fast_and_Accurate_Shift-Reduce_Constituent_Parsing)

Show abstract

[Neural Machine Translation by Jointly Learning to Align and Translate](publication/265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate)

Article

Full-text available

*   Sep 2014

*   [![Dzmitry Bahdanau](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Dzmitry Bahdanau](https://www.researchgate.net/profile/Dzmitry-Bahdanau)
*   [Kyunghyun Cho](https://www.researchgate.net/scientific-contributions/Kyunghyun-Cho-69952266)
*   [![Y. Bengio](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Y. Bengio](https://www.researchgate.net/profile/Y-Bengio)

Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.

[View](publication/265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate)

Show abstract

[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](publication/262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation)

Article

Full-text available

*   Jun 2014

*   [Kyunghyun Cho](https://www.researchgate.net/scientific-contributions/Kyunghyun-Cho-69952266)
*   [Bart van Merriënboer](https://www.researchgate.net/scientific-contributions/Bart-van-Merrienboer-2049482566)
*   [![Caglar Gulcehre](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Caglar Gulcehre](https://www.researchgate.net/profile/Caglar-Gulcehre)
*   [![Y. Bengio](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Y. Bengio](https://www.researchgate.net/profile/Y-Bengio)

In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.

[View](publication/262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation)

Show abstract

[Effective self-training for parsing](publication/262408350_Effective_self-training_for_parsing)

Conference Paper

Full-text available

*   Jun 2006

*   [![David McClosky](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)David McClosky](https://www.researchgate.net/profile/David-Mcclosky-2)
*   [Eugene Charniak](https://www.researchgate.net/scientific-contributions/Eugene-Charniak-62095226)
*   [![Mark Johnson](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Mark Johnson](https://www.researchgate.net/profile/Mark-Johnson-3)

We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.

[View](publication/262408350_Effective_self-training_for_parsing)

Show abstract

[Self-Training PCFG Grammars with Latent Annotations Across Languages.](publication/221013135_Self-Training_PCFG_Grammars_with_Latent_Annotations_Across_Languages)

Conference Paper

Full-text available

*   Jan 2009

*   [Zhongqiang Huang](https://www.researchgate.net/scientific-contributions/Zhongqiang-Huang-33263112)
*   [![Mary Harper](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Mary Harper](https://www.researchgate.net/profile/Mary-Harper)

We investigate the effectiveness of self- training PCFG grammars with latent anno- tations (PCFG-LA) for parsing languages with different amounts of labeled training data. Compared to Charniak's lexicalized parser, the PCFG-LA parser was more ef- fectively adapted to a language for which parsing has been less well developed (i.e., Chinese) and benefited more from self- training. We show for the first time that self-training is able to significantly im- prove the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled train- ing data. Our approach achieves state- of-the-art parsing accuracies for a single parser on both English (91.5%) and Chi- nese (85.2%).

[View](publication/221013135_Self-Training_PCFG_Grammars_with_Latent_Annotations_Across_Languages)

Show abstract

[Learning Accurate, Compact, and Interpretable Tree Annotation.](publication/220873863_Learning_Accurate_Compact_and_Interpretable_Tree_Annotation)

Conference Paper

Full-text available

*   Jan 2006

*   [Slav Petrov](https://www.researchgate.net/scientific-contributions/Slav-Petrov-69706119)
*   [![Leon Barrett](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Leon Barrett](https://www.researchgate.net/profile/Leon-Barrett)
*   [Romain Thibaux](https://www.researchgate.net/scientific-contributions/Romain-Thibaux-70247412)
*   [Dan Klein](https://www.researchgate.net/scientific-contributions/Dan-Klein-70478569)

We present an automatic approach to tree annota- tion in which basic nonterminal symbols are alter- nately split and merged to maximize the likelihood of a training treebank. Starting with a simple X- bar grammar, we learn a new grammar whose non- terminals are subsymbols of the original nontermi- nals. In contrast with previous work, we are able to split various terminals to different degrees, as ap- propriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more ac- curate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.

[View](publication/220873863_Learning_Accurate_Compact_and_Interpretable_Tree_Annotation)

Show abstract

[Long Short-Term Memory](publication/13853244_Long_Short-Term_Memory)

Article

Full-text available

*   Nov 1997
*   [NEURAL COMPUT](journal/Neural-Computation-1530-888X)

*   [![Sepp Hochreiter](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Sepp Hochreiter](https://www.researchgate.net/profile/Sepp-Hochreiter)
*   [Jürgen Schmidhuber](https://www.researchgate.net/scientific-contributions/Juergen-Schmidhuber-40000894)

Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.

[View](publication/13853244_Long_Short-Term_Memory)

Show abstract

[Massive Exploration of Neural Machine Translation Architectures](publication/322584145_Massive_Exploration_of_Neural_Machine_Translation_Architectures)

Conference Paper

*   Jan 2017

*   [Denny Britz](https://www.researchgate.net/scientific-contributions/Denny-Britz-2120630203)
*   [![Anna Helen Goldie](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Anna Helen Goldie](https://www.researchgate.net/profile/Anna-Goldie)
*   [Minh-Thang Luong](https://www.researchgate.net/scientific-contributions/Minh-Thang-Luong-2079824147)
*   [Quoc Le](https://www.researchgate.net/scientific-contributions/Quoc-Le-2296645770)

[View](publication/322584145_Massive_Exploration_of_Neural_Machine_Translation_Architectures)

[Xception: Deep Learning with Depthwise Separable Convolutions](publication/320968382_Xception_Deep_Learning_with_Depthwise_Separable_Convolutions)

Conference Paper

*   Jul 2017

*   [Francois Chollet](https://www.researchgate.net/scientific-contributions/Francois-Chollet-2111010113)

[View](publication/320968382_Xception_Deep_Learning_with_Depthwise_Separable_Convolutions)

[Sequence to Sequence Learning with Neural Networks](publication/319770465_Sequence_to_Sequence_Learning_with_Neural_Networks)

Conference Paper

*   Sep 2014

*   [Ilya Sutskever](https://www.researchgate.net/scientific-contributions/Ilya-Sutskever-16196090)
*   [Oriol Vinyals](https://www.researchgate.net/scientific-contributions/Oriol-Vinyals-69685462)
*   [Quoc V. Le](https://www.researchgate.net/scientific-contributions/Quoc-V-Le-35179114)

Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.7 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a strong phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which beats the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.

[View](publication/319770465_Sequence_to_Sequence_Learning_with_Neural_Networks)

Show abstract

[Neural GPUs Learn Algorithms](publication/319770135_Neural_GPUs_Learn_Algorithms)

Conference Paper

*   Nov 2016

*   [Lukasz Kaiser](https://www.researchgate.net/scientific-contributions/Lukasz-Kaiser-2129002668)
*   [Ilya Sutskever](https://www.researchgate.net/scientific-contributions/Ilya-Sutskever-16196090)

Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.

[View](publication/319770135_Neural_GPUs_Learn_Algorithms)

Show abstract

[Building a large annotated corpus of english: The penn treebank](publication/312607081_Building_a_large_annotated_corpus_of_english_The_penn_treebank)

Article

*   Jan 1994

*   [![Mitchell Marcus](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Mitchell Marcus](https://www.researchgate.net/profile/Mitchell-Marcus-2)
*   [Beatrice Santorini](https://www.researchgate.net/scientific-contributions/Beatrice-Santorini-2111185594)
*   [M.A. Marcinkiewicz](https://www.researchgate.net/scientific-contributions/MA-Marcinkiewicz-2107336688)

[View](publication/312607081_Building_a_large_annotated_corpus_of_english_The_penn_treebank)

[Long Short-Term Memory-Networks for Machine Reading](publication/312416396_Long_Short-Term_Memory-Networks_for_Machine_Reading)

Conference Paper

*   Jan 2016

*   [Jianpeng Cheng](https://www.researchgate.net/scientific-contributions/Jianpeng-Cheng-2108137820)
*   [![Li Dong](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Li Dong](https://www.researchgate.net/profile/Li-Dong-12)
*   [![Mirella Lapata](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Mirella Lapata](https://www.researchgate.net/profile/Mirella-Lapata-2)

[View](publication/312416396_Long_Short-Term_Memory-Networks_for_Machine_Reading)

[Can Active Memory Replace Attention?](publication/309484464_Can_Active_Memory_Replace_Attention)

Article

*   Oct 2016

*   [Lukasz Kaiser](https://www.researchgate.net/scientific-contributions/Lukasz-Kaiser-2129002668)
*   [Samy Bengio](https://www.researchgate.net/scientific-contributions/Samy-Bengio-67738097)

Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.

[View](publication/309484464_Can_Active_Memory_Replace_Attention)

Show abstract

[Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translatin](publication/303969947_Deep_Recurrent_Models_with_Fast-Forward_Connections_for_Neural_Machine_Translatin)

Article

*   Jun 2016

*   [Jie Zhou](https://www.researchgate.net/scientific-contributions/Jie-Zhou-2127101155)
*   [Ying Cao](https://www.researchgate.net/scientific-contributions/Ying-Cao-2111022485)
*   [Xuguang Wang](https://www.researchgate.net/scientific-contributions/Xuguang-Wang-2162957931)
*   [Wei Xu](https://www.researchgate.net/scientific-contributions/Wei-Xu-2193042422)

Neural machine translation (NMT) aims at solving machine translation (MT) problems with purely neural networks and exhibits promising results in recent years. However, most of the existing NMT models are of shallow topology and there is still a performance gap between the single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) network, together with the interleaved bi-directional way for stacking them. Fast-forward connections play an essential role to propagate the gradients in building the deep topology of depth 16. On WMT'14 English- to-French task, we achieved BLEU=37.7 with single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. It is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. Even without considering attention mechanism, we can still achieve BLEU=36.3. After the special handling for unknown words and the model ensemble, we obtained the best score on this task with BLEU=40.4. Our models are also verified on the more difficult WMT'14 English-to-German task.

[View](publication/303969947_Deep_Recurrent_Models_with_Fast-Forward_Connections_for_Neural_Machine_Translatin)

Show abstract

[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](publication/286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting)

Article

*   Jun 2014
*   J MACH LEARN RES

*   [![Nitish Srivastava](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Nitish Srivastava](https://www.researchgate.net/profile/Nitish-Srivastava-7)
*   [Geoffrey E. Hinton](https://www.researchgate.net/scientific-contributions/Geoffrey-E-Hinton-6458213)
*   [Alex Krizhevsky](https://www.researchgate.net/scientific-contributions/Alex-Krizhevsky-69915774)
*   [Ruslan Salakhutdinov](https://www.researchgate.net/scientific-contributions/Ruslan-Salakhutdinov-71111877)

Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. © 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.

[View](publication/286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting)

Show abstract

[Rethinking the Inception Architecture for Computer Vision](publication/285648386_Rethinking_the_Inception_Architecture_for_Computer_Vision)

Article

*   Dec 2015

*   [Christian Szegedy](https://www.researchgate.net/scientific-contributions/Christian-Szegedy-2039253110)
*   [Vincent Vanhoucke](https://www.researchgate.net/scientific-contributions/Vincent-Vanhoucke-2022388739)
*   [Sergey Ioffe](https://www.researchgate.net/scientific-contributions/Sergey-Ioffe-2039952987)
*   [![Zbigniew Wojna](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)Zbigniew Wojna](https://www.researchgate.net/profile/Zbigniew-Wojna)

Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error and 17.3% top-1 error.

[View](publication/285648386_Rethinking_the_Inception_Architecture_for_Computer_Vision)

Show abstract

[Grammar as a Foreign Language](publication/269997813_Grammar_as_a_Foreign_Language)

Article

*   Dec 2014

*   [Oriol Vinyals](https://www.researchgate.net/scientific-contributions/Oriol-Vinyals-69685462)
*   [Lukasz Kaiser](https://www.researchgate.net/scientific-contributions/Lukasz-Kaiser-2129002668)
*   [Terry Koo](https://www.researchgate.net/scientific-contributions/Terry-Koo-2061532622)
*   [Geoffrey E. Hinton](https://www.researchgate.net/scientific-contributions/Geoffrey-E-Hinton-6458213)

Syntactic parsing is a fundamental problem in computational linguistics and Natural Language Processing. Traditional approaches to parsing are highly complex and problem specific. Recently, Sutskever et al. (2014) presented a domain-independent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem. In this work, we show that precisely the same sequence-to-sequence method achieves results that are close to state-of-the-art on syntactic constituency parsing, whilst making almost no assumptions about the structure of the problem.

[View](publication/269997813_Grammar_as_a_Foreign_Language)

Show abstract

[Adam: A Method for Stochastic Optimization](publication/269935079_Adam_A_Method_for_Stochastic_Optimization)

Article

*   Dec 2014

*   [Diederik Kingma](https://www.researchgate.net/scientific-contributions/Diederik-Kingma-2061341099)
*   [Jimmy Ba](https://www.researchgate.net/scientific-contributions/Jimmy-Ba-2066541611)

We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based an adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also ap- propriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice when experimentally compared to other stochastic optimization methods.

[View](publication/269935079_Adam_A_Method_for_Stochastic_Optimization)

Show abstract

*   Jan 2016

*   Jimmy Lei Ba
*   Jamie Ryan Kiros
*   Geoffrey E Hinton

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

Convolutional sequence to sequence learning

*   Jan 2017

*   Jonas Gehring
*   Michael Auli
*   David Grangier
*   Denis Yarats
*   Yann N Dauphin

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.

Generating sequences with recurrent neural networks

*   Jan 2013

*   Alex Graves

Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.

Gradient flow in recurrent nets: the difficulty of learning long-term dependencies

*   Jan 2001

*   Sepp Hochreiter
*   Yoshua Bengio
*   Paolo Frasconi
*   Jürgen Schmidhuber

Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.

Exploring the limits of language modeling

*   Jan 2016

*   Rafal Jozefowicz
*   Oriol Vinyals
*   Mike Schuster
*   Noam Shazeer
*   Yonghui Wu

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.

*   Jan 2017

*   Nal Kalchbrenner
*   Lasse Espeholt
*   Karen Simonyan
*   Aaron Van Den Oord
*   Alex Graves
*   Koray Kavukcuoglu

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.

Structured attention networks

*   Jan 2017

*   Yoon Kim
*   Carl Denton
*   Luong Hoang
*   Alexander M Rush

Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.

*   Jan 2017

*   Oleksii Kuchaiev
*   Boris Ginsburg

Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.

A structured self-attentive sentence embedding

*   Jan 2017

*   Zhouhan Lin
*   Minwei Feng
*   Cicero Nogueira
*   Mo Santos
*   Bing Yu
*   Bowen Xiang
*   Yoshua Zhou
*   Bengio

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.

Effective approaches to attentionbased neural machine translation

*   Jan 2015

*   Minh-Thang Luong
*   Hieu Pham
*   Christopher D Manning

Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.

A decomposable attention model

*   Jan 2016

*   Ankur Parikh
*   Oscar Täckström
*   Dipanjan Das
*   Jakob Uszkoreit

Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.

A deep reinforced model for abstractive summarization

*   Jan 2017

*   Romain Paulus
*   Caiming Xiong
*   Richard Socher

Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.

*   Jan 2015

*   Rico Sennrich
*   Barry Haddow
*   Alexandra Birch

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.

Outrageously large neural networks: The sparsely-gated mixture-of-experts layer

*   Jan 2017

*   Noam Shazeer
*   Azalia Mirhoseini
*   Krzysztof Maziarz
*   Andy Davis
*   Quoc Le
*   Geoffrey Hinton
*   Jeff Dean

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

Google's neural machine translation system: Bridging the gap between human and machine translation

*   Jan 2016

*   Yonghui Wu
*   Mike Schuster
*   Zhifeng Chen
*   V Quoc
*   Mohammad Le
*   Wolfgang Norouzi
*   Maxim Macherey
*   Yuan Krikun
*   Qin Cao
*   Klaus Gao
*   Macherey

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.

## Recommended publications

[Discover more](search)

Article

### [Translational Neurocardiology: preclinical models and cardioneural integrative aspects](publication/301563461_Translational_Neurocardiology_preclinical_models_and_cardioneural_integrative_aspects)

April 2016 · The Journal of Physiology

*   ![Jeffrey L. Ardell](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)[Jeffrey L. Ardell](https://www.researchgate.net/profile/Jeffrey-Ardell-2)
*   ![Michael C Andresen](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)[Michael C Andresen](https://www.researchgate.net/profile/Michael-Andresen-3)
*   [J A Armour](https://www.researchgate.net/scientific-contributions/J-A-Armour-38940269)
*   \[...\]
*   ![Irving H Zucker](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)[Irving H Zucker](https://www.researchgate.net/profile/Irving-Zucker-2)

Neuronal elements distributed throughout the cardiac nervous system, from the level of the insular cortex to the intrinsic cardiac nervous system, are in constant communication with one another to assure that cardiac output matches the dynamic process of regional blood flow demand. Neural elements in their various 'levels' become differentially recruited in the transduction of sensory inputs ... \[Show full abstract\] arising from the heart, major vessels, other visceral organs and somatic structures to optimize neuronal coordination of regional cardiac function. Figure 1 presents the contextual framework for these interactions. This white paper will review the relevant aspects of the structural and functional organization for autonomic control of the heart in normal conditions, how these systems remodel/adapt during cardiac disease, and finally how such knowledge can be leveraged in the evolving realm of autonomic regulation therapy for cardiac therapeutics. This article is protected by copyright. All rights reserved.

[Read more](publication/301563461_Translational_Neurocardiology_preclinical_models_and_cardioneural_integrative_aspects)

Article

### [SC - DL tentative](publication/234902627_SC_-_DL_tentative)

October 1990

*   [Charles J. Lloyd](https://www.researchgate.net/scientific-contributions/Charles-J-Lloyd-2044492381)
*   [Robert J. Beaton](https://www.researchgate.net/scientific-contributions/Robert-J-Beaton-2005416510)

Two general characteristics of full-color display systems which are known to impact image quality include the ability of the display system to transfer modulation (chromatic as well as achromatic) and the degree to which the display system adds noise (chromatic and achromatic) to the signal. This paper describes a model of human spatial-chromatic vision and a corresponding procedure for using the ... \[Show full abstract\] model to evaluate color display systems. Together the proposed model and procedure constitute a color image quality metric which is responsive to the modulation transfer and noise generating characteristics of a display system. The proposed human vision model employs processing stages which simulate blurring by the optics of the eye, linear spectral absorption by three classes of cone, addition of internal noise, nonlinear transduction by retinal mechanisms, derivation of opponent-color images, and calculation of the responses of linear spatial mechanisms with finite spatial frequency and orientation bandwidth. A summary of the modulation detection, discrimination, and suprathreshold contrast perception performance of the model is presented and compared with human performance data from the visual science literature. A procedure for evaluating display systems using the model is described and the results of several analyses of display systems are presented. High correlations between predictions made by the model and the results of image quality studies from the display design literature have been obtained with no free parameters in the model. The results of the validation studies conducted so far suggest that the proposed method for evaluating color display systems is viable and warrants critical examination.© (1990) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.

[Read more](publication/234902627_SC_-_DL_tentative)

Article

### [Hypothesis Pruning in Learning Word Alignment](publication/289817455_Hypothesis_Pruning_in_Learning_Word_Alignment)

January 2013 · Chinese Journal of Electronics

*   [S. Huang](https://www.researchgate.net/scientific-contributions/S-Huang-2093061201)
*   [X. Dai](https://www.researchgate.net/scientific-contributions/X-Dai-2093138032)
*   [Jiajun Chen](https://www.researchgate.net/scientific-contributions/Jiajun-Chen-2093001663)

Recent study shows that discriminative learning methods could provide a significant improvement of word alignment quality. One of the difficulties of these methods is how to perform efficient search of word alignment. Although Inversion transduction grammar (ITG) provides a polynomial time algorithm using synchronous parsing techniques, a very harsh pruning is still needed to make the algorithm ... \[Show full abstract\] computationally feasible. We notice that previous pruning techniques mostly focus on pruning the bi-lingual spans, after what low quality alignments still exist. To address this problem, we propose an approach that prunes low quality hypotheses on-the-fly during parsing. Compared with previous pruning methods which only use high precision alignment links as constraints, our method could make use of "high recall" alignment links as well. To demonstrate our approach, we also propose a constrained learning framework, which generates high precision and high recall constraints from some existing alignment results. Experiment shows significant improvements of both alignment and translation quality over standard IBM Model 4 alignments on the Chinese-English test data.

[Read more](publication/289817455_Hypothesis_Pruning_in_Learning_Word_Alignment)

Conference Paper

Full-text available

### [Efficient Search for Inversion Transduction Grammar.](publication/221012607_Efficient_Search_for_Inversion_Transduction_Grammar)

January 2006

*   ![Hao Zhang](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)[Hao Zhang](https://www.researchgate.net/profile/Hao-Zhang-187)
*   [Daniel Gildea](https://www.researchgate.net/scientific-contributions/Daniel-Gildea-11407952)

We develop admissible A\* search heuris- tics for synchronous parsing with Inver- sion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding. We also combine the dynamic programming hook trick with A\* search for decoding. These techniques make it possible to nd opti- mal alignments much more quickly, and make it possible to nd optimal transla- ... \[Show full abstract\] tions for the rst time. Even in the pres- ence of pruning, we are able to achieve higher BLEU scores with the same amount of computation.

[View full-text](publication/221012607_Efficient_Search_for_Inversion_Transduction_Grammar)

## Looking for the full-text?

You can request the full-text of this article directly from the authors on ResearchGate.

Request full-text

Already a member? [Log in](login)

[![RG Logo](https://www.researchgate.net/images/nativeApp/rg_app_ios_logo_small.png)](go.GetApp.html?_sg=aokQgK-Ce-2Wj-9DSDuHTNDXx8goRXzpnakDV_waLXw6OMATgG5hU8L_BEJakI1rdl2m6AKTwQDBZPKPrqUpg00BX4iZ5rgT3oNmo9M1n8Q&originCh=bannerStatsCopy&relativePath=publicationLoggedOut&interested=true)

[

**ResearchGate iOS App**

Get it from the App Store now.





](go.GetApp.html?_sg=aokQgK-Ce-2Wj-9DSDuHTNDXx8goRXzpnakDV_waLXw6OMATgG5hU8L_BEJakI1rdl2m6AKTwQDBZPKPrqUpg00BX4iZ5rgT3oNmo9M1n8Q&originCh=bannerStatsCopy&relativePath=publicationLoggedOut&interested=true)

[Install](go.GetApp.html?_sg=aokQgK-Ce-2Wj-9DSDuHTNDXx8goRXzpnakDV_waLXw6OMATgG5hU8L_BEJakI1rdl2m6AKTwQDBZPKPrqUpg00BX4iZ5rgT3oNmo9M1n8Q&originCh=bannerStatsCopy&relativePath=publicationLoggedOut&interested=true)

[

Keep up with your stats and more





](go.GetApp.html?_sg=aokQgK-Ce-2Wj-9DSDuHTNDXx8goRXzpnakDV_waLXw6OMATgG5hU8L_BEJakI1rdl2m6AKTwQDBZPKPrqUpg00BX4iZ5rgT3oNmo9M1n8Q&originCh=bannerStatsCopy&relativePath=publicationLoggedOut&interested=true)

[

Access scientific knowledge from anywhere





](go.GetApp.html?_sg=aokQgK-Ce-2Wj-9DSDuHTNDXx8goRXzpnakDV_waLXw6OMATgG5hU8L_BEJakI1rdl2m6AKTwQDBZPKPrqUpg00BX4iZ5rgT3oNmo9M1n8Q&originCh=bannerStatsCopy&relativePath=publicationLoggedOut&interested=true)

[![ResearchGate Logo](images/icons/svgicons/researchgate-logo-white.svg)](https://www.researchgate.net/)

or

[Discover by subject area](topics)

*   [Recruit researchers](scientific-recruitment/?utm_source=researchgate&utm_medium=community-loggedout&utm_campaign=indextop)
*   [Join for free](signup.SignUp.html?hdrsu=1&_sg%5B0%5D=JjeBhterLih-D7Ork7OxUX4e44iotjLFjdjA3kUccn603iDlYeaX9Q-pFgdpfELhF1Mt7qzzfQXN-ArUq1Op_vC-1D4)
*   Login
    
    Email
    
    **Tip:** Most researchers use their institutional email address as their ResearchGate login
    
    Password[Forgot password?](application.LostPassword.html)
    
     Keep me logged in
    
    Log in
    
    or
    
    [
    
    Continue with Google
    
    ](connector/google)
    
    Welcome back! Please log in.
    
    Email
    
    · Hint
    
    **Tip:** Most researchers use their institutional email address as their ResearchGate login
    
    Password[Forgot password?](application.LostPassword.html)
    
     Keep me logged in
    
    Log in
    
    or
    
    [
    
    Continue with Google
    
    ](connector/google)
    
    No account? [Sign up](signup.SignUp.html?hdrsu=1&_sg%5B0%5D=JjeBhterLih-D7Ork7OxUX4e44iotjLFjdjA3kUccn603iDlYeaX9Q-pFgdpfELhF1Mt7qzzfQXN-ArUq1Op_vC-1D4)
    

[![App Store](https://i1.rgstatic.net/images/nativeApp/app_store_dark_logo.svg)](go.GetApp.html?interested=true&originCh=footerLoggedOut)

[![Get it on Google Play](https://c5.rgstatic.net/m/44490443524024/images/appstore/AndroidCta.svg)](deref/https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dnet.researchgate.shellapp%26utm_source%3DoriginCH%253Dfooter%26pcampaignid%3DpcampaignidMKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1)

Company

[About us](about)

[News](blog)

[Careers](careers)

Support

[Help Center](https://explore.researchgate.net/?utm_source=researchgate&utm_medium=community-loggedout&utm_campaign=new-footer&utm_content=helpcenter)

Business solutions

[Advertising](https://www.researchgate.net/marketing-solutions?utm_source=researchgate&utm_medium=community-loggedout&utm_campaign=new-footer&utm_content=advertising)

[Recruiting](scientific-recruitment?utm_source=researchgate&utm_medium=community-loggedout&utm_campaign=new-footer&utm_content=recruiting)

© 2008-2025 ResearchGate GmbH. All rights reserved.

*   [Terms](terms-of-service)
*   [Privacy](privacy-policy)
*   [Copyright](ip-policy)
*   [Imprint](imprint)
*   [Consent preferences](javascript:Didomi.preferences.show\(\))

        

**We and our partners use cookies**✕

By using this site, you consent to the processing of your personal data, the storing of cookies on your device, and the use of similar technologies for personali